{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def scale_features(X, scalers):\n",
    "    X_scaled = X.copy()\n",
    "    for feature in X.columns:\n",
    "        X_scaled[feature] = scalers[feature].transform(X[[feature]])\n",
    "    return X_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T23:33:26.567512Z",
     "start_time": "2025-02-26T23:33:26.560986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-26T23:38:04.422537Z",
     "start_time": "2025-02-26T23:33:26.604783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Model: EBM -- Fold: 1/5 -----\n",
      "Best hyperparameters: {'max_bins': 256, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'random_state': 42}\n",
      "\n",
      "----- Model: EBM -- Fold: 2/5 -----\n",
      "Best hyperparameters: {'max_bins': 512, 'interactions': 20, 'outer_bags': 16, 'inner_bags': 4, 'random_state': 42}\n",
      "\n",
      "----- Model: EBM -- Fold: 3/5 -----\n",
      "Best hyperparameters: {'max_bins': 512, 'interactions': 20, 'outer_bags': 16, 'inner_bags': 4, 'random_state': 42}\n",
      "\n",
      "----- Model: EBM -- Fold: 4/5 -----\n",
      "Best hyperparameters: {'max_bins': 512, 'interactions': 20, 'outer_bags': 16, 'inner_bags': 4, 'random_state': 42}\n",
      "\n",
      "----- Model: EBM -- Fold: 5/5 -----\n",
      "Best hyperparameters: {'max_bins': 256, 'interactions': 20, 'outer_bags': 16, 'inner_bags': 0, 'random_state': 42}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73       819\n",
      "           1       0.70      0.73      0.71       750\n",
      "\n",
      "    accuracy                           0.72      1569\n",
      "   macro avg       0.72      0.72      0.72      1569\n",
      "weighted avg       0.72      0.72      0.72      1569\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "['scalers.pkl']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, classification_report\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from itertools import product\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"heloc_preprocessed.csv\")\n",
    "\n",
    "simple_feature_names = [\"Overall Credit Risk Score\", \"Months Since First Credit Account\", \"Average Age of Credit Accounts\", \"Number of Well-Maintained Accounts\", \"Percentage of Accounts Never Late\",\n",
    "                            \"Months Since Last Missed Payment\", \"Percentage of Installment vs Revolving Loans\", \"Time Since Last Credit Application\", \"Credit Utilization Ratio\", \"Number of Active Credit Cards/Lines\", \"Loan Repaid\"]\n",
    "\n",
    "df_simple = df.copy()\n",
    "df_simple.columns = simple_feature_names\n",
    "\n",
    "y = df_simple[\"Loan Repaid\"]\n",
    "X = df_simple.drop(columns=\"Loan Repaid\")\n",
    "\n",
    "scalers = {feature: StandardScaler() for feature in X.columns}\n",
    "\n",
    "n_folds = 5\n",
    "random_state = 42\n",
    "\n",
    "model_name = \"EBM\"\n",
    "# Hyperparameter search space\n",
    "ebm_hyperparameters = {\n",
    "    \"max_bins\": [256, 512],\n",
    "    \"interactions\": [0, 10, 20],\n",
    "    \"outer_bags\": [8, 16],\n",
    "    \"inner_bags\": [0, 4],\n",
    "    \"random_state\": [random_state]\n",
    "}\n",
    "\n",
    "overall_best_hp_config = None\n",
    "overall_best_loss = np.inf\n",
    "best_model = None  # Store the best model overall\n",
    "\n",
    "# Split off final test set for dashboard accuracy calculation\n",
    "X_train_full, X_final_test, y_train_full, y_final_test = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=random_state\n",
    ")\n",
    "\n",
    "for feature in X.columns:\n",
    "    scalers[feature].fit(X_train_full[[feature]])  # Only fit on the training set\n",
    "\n",
    "X_train_full_scaled = scale_features(X_train_full, scalers)\n",
    "X_final_test_scaled = scale_features(X_final_test, scalers)\n",
    "\n",
    "# Use KFold (not stratified) since dataset is fairly balanced\n",
    "outer_cv = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "for fold_i, (train_val_idx, test_idx) in enumerate(outer_cv.split(X_train_full_scaled, y_train_full)):\n",
    "    print(f\"\\n----- Model: {model_name} -- Fold: {fold_i + 1}/{n_folds} -----\")\n",
    "\n",
    "    X_train_val_scaled, y_train_val = X_train_full_scaled.iloc[train_val_idx], y_train_full.iloc[train_val_idx]\n",
    "    X_test_scaled, y_test = X_train_full_scaled.iloc[test_idx], y_train_full.iloc[test_idx]\n",
    "\n",
    "    # Split the training data into train and validation sets\n",
    "    X_train_scaled, X_val_scaled, y_train, y_val = train_test_split(\n",
    "        X_train_val_scaled, y_train_val, test_size=0.25, stratify=y_train_val, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Grid search over hyperparameters\n",
    "    best_hp_config = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for hp in product(*ebm_hyperparameters.values()):\n",
    "        params = dict(zip(ebm_hyperparameters.keys(), hp))\n",
    "        model = ExplainableBoostingClassifier(**params)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        y_val_pred_proba = model.predict_proba(X_val_scaled)\n",
    "        ce_loss = log_loss(y_val, y_val_pred_proba)\n",
    "\n",
    "        if ce_loss < best_loss:\n",
    "            best_loss = ce_loss\n",
    "            best_hp_config = params\n",
    "            best_model = model  # Store the best model found\n",
    "\n",
    "    # After the grid search, we select the best model and print hyperparameters\n",
    "    print(f\"Best hyperparameters: {best_hp_config}\")\n",
    "\n",
    "    # Inside the cross-validation loop, update this:\n",
    "    if best_loss < overall_best_loss:\n",
    "        overall_best_loss = best_loss\n",
    "        overall_best_hp_config = best_hp_config  # Store best config across folds\n",
    "\n",
    "# Retain the best-trained model instead of retraining from scratch\n",
    "final_model = best_model  # Use the best model found during CV\n",
    "\n",
    "# Predict on final test set\n",
    "y_final_pred = final_model.predict(X_final_test_scaled)\n",
    "\n",
    "# Compute accuracy & metrics\n",
    "print(classification_report(y_final_test, y_final_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(final_model, \"final_ebm_standardized_model.pkl\")\n",
    "joblib.dump(scalers, \"scalers.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Model: EBM -- Fold: 1/5 -----\n",
      "Best hyperparameters: {'max_bins': 256, 'interactions': 10, 'outer_bags': 8, 'inner_bags': 4, 'random_state': 42}\n",
      "\n",
      "----- Model: EBM -- Fold: 2/5 -----\n",
      "Best hyperparameters: {'max_bins': 512, 'interactions': 20, 'outer_bags': 16, 'inner_bags': 4, 'random_state': 42}\n",
      "\n",
      "----- Model: EBM -- Fold: 3/5 -----\n",
      "Best hyperparameters: {'max_bins': 512, 'interactions': 20, 'outer_bags': 16, 'inner_bags': 4, 'random_state': 42}\n",
      "\n",
      "----- Model: EBM -- Fold: 4/5 -----\n",
      "Best hyperparameters: {'max_bins': 512, 'interactions': 20, 'outer_bags': 16, 'inner_bags': 4, 'random_state': 42}\n",
      "\n",
      "----- Model: EBM -- Fold: 5/5 -----\n",
      "Best hyperparameters: {'max_bins': 256, 'interactions': 20, 'outer_bags': 16, 'inner_bags': 0, 'random_state': 42}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73       819\n",
      "           1       0.70      0.73      0.71       750\n",
      "\n",
      "    accuracy                           0.72      1569\n",
      "   macro avg       0.72      0.72      0.72      1569\n",
      "weighted avg       0.72      0.72      0.72      1569\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "['final_ebm_model.pkl']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import log_loss, classification_report\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from itertools import product\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"heloc_preprocessed.csv\")\n",
    "\n",
    "simple_feature_names = [\"Overall Credit Risk Score\", \"Months Since First Credit Account\", \"Average Age of Credit Accounts\", \"Number of Well-Maintained Accounts\", \"Percentage of Accounts Never Late\",\n",
    "                            \"Months Since Last Missed Payment\", \"Percentage of Installment vs Revolving Loans\", \"Time Since Last Credit Application\", \"Credit Utilization Ratio\", \"Number of Active Credit Cards/Lines\", \"Loan Repaid\"]\n",
    "\n",
    "df_simple = df.copy()\n",
    "df_simple.columns = simple_feature_names\n",
    "\n",
    "y = df_simple[\"Loan Repaid\"]\n",
    "X = df_simple.drop(columns=\"Loan Repaid\")\n",
    "\n",
    "n_folds = 5\n",
    "random_state = 42\n",
    "\n",
    "model_name = \"EBM\"\n",
    "# Hyperparameter search space\n",
    "ebm_hyperparameters = {\n",
    "    \"max_bins\": [256, 512],\n",
    "    \"interactions\": [0, 10, 20],\n",
    "    \"outer_bags\": [8, 16],\n",
    "    \"inner_bags\": [0, 4],\n",
    "    \"random_state\": [random_state]\n",
    "}\n",
    "\n",
    "overall_best_hp_config = None\n",
    "overall_best_loss = np.inf\n",
    "best_model = None  # Store the best model overall\n",
    "\n",
    "# Split off final test set for dashboard accuracy calculation\n",
    "X_train_full, X_final_test, y_train_full, y_final_test = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=random_state\n",
    ")\n",
    "\n",
    "# Use KFold (not stratified) since dataset is fairly balanced\n",
    "outer_cv = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "for fold_i, (train_val_idx, test_idx) in enumerate(outer_cv.split(X_train_full, y_train_full)):\n",
    "    print(f\"\\n----- Model: {model_name} -- Fold: {fold_i + 1}/{n_folds} -----\")\n",
    "\n",
    "    # Split train-validation-test\n",
    "    X_train_val, y_train_val = X_train_full.iloc[train_val_idx], y_train_full.iloc[train_val_idx]\n",
    "    X_test, y_test = X_train_full.iloc[test_idx], y_train_full.iloc[test_idx]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Grid search over hyperparameters\n",
    "    best_hp_config = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for hp in product(*ebm_hyperparameters.values()):\n",
    "        params = dict(zip(ebm_hyperparameters.keys(), hp))\n",
    "        model = ExplainableBoostingClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_val_pred_proba = model.predict_proba(X_val)\n",
    "        ce_loss = log_loss(y_val, y_val_pred_proba)\n",
    "\n",
    "        if ce_loss < best_loss:\n",
    "            best_loss = ce_loss\n",
    "            best_hp_config = params\n",
    "            best_model = model  # Store the best model found\n",
    "\n",
    "    # Train final model on full train-val data with best params\n",
    "    print(f\"Best hyperparameters: {best_hp_config}\")\n",
    "\n",
    "    # Inside the cross-validation loop, update this:\n",
    "    if best_loss < overall_best_loss:\n",
    "        overall_best_loss = best_loss\n",
    "        overall_best_hp_config = best_hp_config  # Store best config across folds\n",
    "\n",
    "# Retain the best-trained model instead of retraining from scratch\n",
    "final_model2 = best_model  # Use the best model found during CV\n",
    "\n",
    "# Predict on final test set\n",
    "y_final_pred = final_model2.predict(X_final_test)\n",
    "\n",
    "# Compute accuracy & metrics\n",
    "print(classification_report(y_final_test, y_final_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(final_model2, \"final_ebm_model.pkl\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T23:42:11.333575Z",
     "start_time": "2025-02-26T23:38:04.422161Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/html": "<!-- http://127.0.0.1:7001/140446342494432/ -->\n<iframe src=\"http://127.0.0.1:7001/140446342494432/\" width=100% height=800 frameBorder=\"0\"></iframe>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interpret import show\n",
    "ebm_standard = joblib.load(\"final_ebm_standardized_model.pkl\")\n",
    "show(ebm_standard.explain_global())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T23:43:35.676231Z",
     "start_time": "2025-02-26T23:43:34.935867Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/html": "<!-- http://127.0.0.1:7001/140446377469168/ -->\n<iframe src=\"http://127.0.0.1:7001/140446377469168/\" width=100% height=800 frameBorder=\"0\"></iframe>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ebm = joblib.load(\"final_ebm_model.pkl\")\n",
    "show(ebm.explain_global())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T23:43:54.932244Z",
     "start_time": "2025-02-26T23:43:54.885977Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
