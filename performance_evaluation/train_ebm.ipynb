{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"heloc_preprocessed.csv\")\n",
    "\n",
    "simple_feature_names = [\"Overall Credit Risk Score\", \"Months Since First Credit Account\", \"Average Age of Credit Accounts\", \"Number of Well-Maintained Accounts\", \"Percentage of Accounts Never Late\",\n",
    "                            \"Months Since Last Missed Payment\", \"Percentage of Installment vs Revolving Loans\", \"Time Since Last Credit Application\", \"Credit Utilization Ratio\", \"Number of Active Credit Cards/Lines\", \"Loan Repaid\"]\n",
    "\n",
    "df_simple = df.copy()\n",
    "df_simple.columns = simple_feature_names\n",
    "\n",
    "y = df_simple[\"Loan Repaid\"]\n",
    "X = df_simple.drop(columns=\"Loan Repaid\")\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T16:45:21.452110Z",
     "start_time": "2025-02-26T16:45:21.428347Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "ebm = ExplainableBoostingClassifier(random_state=)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, classification_report\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from itertools import product\n",
    "import joblib\n",
    "\n",
    "n_folds = 5\n",
    "random_state = 42\n",
    "\n",
    "model_name = \"EBM\"\n",
    "ebm_hyperparameters = {\n",
    "    \"max_bins\": [256, 512],\n",
    "    \"interactions\": [0, 10, 20],\n",
    "    \"outer_bags\": [8, 16],\n",
    "    \"inner_bags\": [0, 4],\n",
    "}\n",
    "\n",
    "overall_best_hp_config = None\n",
    "overall_best_loss = np.inf\n",
    "\n",
    "# Split off final test set before training\n",
    "X_train_full, X_final_test, y_train_full, y_final_test = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=random_state\n",
    ")\n",
    "\n",
    "# Use KFold for cross-validation\n",
    "outer_cv = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "for fold_i, (train_val_idx, test_idx) in enumerate(outer_cv.split(X_train_full, y_train_full)):\n",
    "    print(f\"\\n----- Model: {model_name} -- Fold: {fold_i + 1}/{n_folds} -----\")\n",
    "\n",
    "    # Split train-validation-test\n",
    "    X_train_val, y_train_val = X_train_full.iloc[train_val_idx], y_train_full.iloc[train_val_idx]\n",
    "    X_test, y_test = X_train_full.iloc[test_idx], y_train_full.iloc[test_idx]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Define preprocessing pipeline\n",
    "    num_pipe = Pipeline([(\"scaler\", StandardScaler())])\n",
    "    ct = ColumnTransformer([(\"num\", num_pipe, X.columns.tolist())])\n",
    "    ct.fit(X_train)\n",
    "\n",
    "    # Transform data\n",
    "    X_train_transformed = pd.DataFrame(ct.transform(X_train), columns=ct.get_feature_names_out())\n",
    "    X_val_transformed = pd.DataFrame(ct.transform(X_val), columns=ct.get_feature_names_out())\n",
    "\n",
    "    # Grid search over hyperparameters\n",
    "    best_hp_config = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for hp in product(*ebm_hyperparameters.values()):\n",
    "        params = dict(zip(ebm_hyperparameters.keys(), hp))\n",
    "        params[\"random_state\"] = random_state  # Ensure consistency\n",
    "\n",
    "        model = ExplainableBoostingClassifier(**params)\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "\n",
    "        y_val_pred_proba = model.predict_proba(X_val_transformed)\n",
    "        ce_loss = log_loss(y_val, y_val_pred_proba)\n",
    "\n",
    "        if ce_loss < best_loss:\n",
    "            best_loss = ce_loss\n",
    "            best_hp_config = params\n",
    "            best_model = model  # Store best model\n",
    "\n",
    "    print(f\"Best hyperparameters for fold {fold_i + 1}: {best_hp_config}\")\n",
    "\n",
    "    # Train final model on full train-val data with best params\n",
    "    X_train_val_transformed = pd.DataFrame(\n",
    "        ct.transform(X_train_val), columns=ct.get_feature_names_out()\n",
    "    )\n",
    "    X_test_transformed = pd.DataFrame(ct.transform(X_test), columns=ct.get_feature_names_out())\n",
    "\n",
    "    final_model = ExplainableBoostingClassifier(**best_hp_config, random_state=random_state)\n",
    "    final_model.fit(X_train_val_transformed, y_train_val)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_test_pred = final_model.predict(X_test_transformed)\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    # Track best overall hyperparameters\n",
    "    if best_loss < overall_best_loss:\n",
    "        overall_best_loss = best_loss\n",
    "        overall_best_hp_config = best_hp_config\n",
    "\n",
    "# Train final model on full training data (excluding X_final_test)\n",
    "final_model = ExplainableBoostingClassifier(**overall_best_hp_config, random_state=random_state)\n",
    "\n",
    "# Standardize using entire training data\n",
    "ct_final = ColumnTransformer([(\"num\", StandardScaler(), X_train_full.columns.tolist())])\n",
    "ct_final.fit(X_train_full)\n",
    "\n",
    "# Transform the training data\n",
    "X_train_full_transformed = pd.DataFrame(\n",
    "    ct_final.transform(X_train_full), columns=ct_final.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Train model\n",
    "final_model.fit(X_train_full_transformed, y_train_full)\n",
    "\n",
    "# Transform final test set\n",
    "X_final_test_transformed = pd.DataFrame(\n",
    "    ct_final.transform(X_final_test), columns=ct_final.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Predict on final test set\n",
    "y_final_pred = final_model.predict(X_final_test_transformed)\n",
    "\n",
    "# Compute final test metrics\n",
    "print(\"\\nFinal Model Evaluation on Hold-Out Test Set:\")\n",
    "print(classification_report(y_final_test, y_final_pred))\n",
    "\n",
    "# Save final model and preprocessor\n",
    "joblib.dump(final_model, \"final_ebm_model.pkl\")\n",
    "joblib.dump(ct_final, \"final_preprocessor.pkl\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
